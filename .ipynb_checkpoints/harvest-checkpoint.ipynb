{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harvest BTAA Geoportals\n",
    "\n",
    "> Original created on Dec 16 2020 <br>\n",
    "Edited on Jan 28 2021 -- Converted GeoJSON into TopoJSON to reduce file size<br>\n",
    "@author: Yijing Zhou @YijingZhou33 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import requests\n",
    "import geopandas as gpd\n",
    "import folium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that **<a href='https://pypi.org/project/mapclassify/'>mapclassify</a>**, **<a href='https://seaborn.pydata.org/'>seaborn</a>** and **<a href='https://mattijn.github.io/topojson/'>topojson</a>** aren't built-in modules in Anaconda. You may need to install in advance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install mapclassify \n",
    "# pip install seaborn\n",
    "# pip install topojson\n",
    "import mapclassify \n",
    "import seaborn as sns \n",
    "import topojson as tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ********** Input Files **********\n",
    "## Raw data: CSV files\n",
    "stategeoportals = os.path.join('data', 'allStates.csv')\n",
    "countygeoportals = os.path.join('data', 'allCounties.csv')\n",
    "citygeoportals = os.path.join('data', 'allCities.csv')\n",
    "\n",
    "## Basemap GeoJSON files for states and counties \n",
    "statejson = os.path.join('data', 'states.json')\n",
    "countyjson = os.path.join('data', 'counties.json')\n",
    "\n",
    "# ********** Output Files **********\n",
    "activestates = os.path.join('json', 'activeStates.topo.json')\n",
    "activecounties = os.path.join('json', 'activeCounties.topo.json')\n",
    "activecities = os.path.join('json', 'activeCities.json')\n",
    "\n",
    "legendjson = os.path.join('json', 'legend.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Part 1: State Geoportals TopoJSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format state name in state geoportals spreadsheet `allStates.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stateCode</th>\n",
       "      <th>State</th>\n",
       "      <th>Title</th>\n",
       "      <th>sourceURL</th>\n",
       "      <th>btaaURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02a-01</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>Illinois Geospatial Data Clearinghouse</td>\n",
       "      <td>https://clearinghouse.isgs.illinois.edu/data</td>\n",
       "      <td>https://geo.btaa.org/?q=02a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02a-03</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>State of Illinois Data Portal</td>\n",
       "      <td>https://data.illinois.gov</td>\n",
       "      <td>https://geo.btaa.org/?q=02a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01d-02</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>Indiana Spatial Data Portal</td>\n",
       "      <td>http://gis.iu.edu</td>\n",
       "      <td>https://geo.btaa.org/?q=01d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>09a-01</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>IndianaMAP</td>\n",
       "      <td>http://maps.indiana.edu/layerGallery.html</td>\n",
       "      <td>https://geo.btaa.org/?q=09a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>03a-02</td>\n",
       "      <td>Iowa</td>\n",
       "      <td>IowaDOT Open Data</td>\n",
       "      <td>https://public-iowadot.opendata.arcgis.com</td>\n",
       "      <td>https://geo.btaa.org/?q=03a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  stateCode     State                                   Title  \\\n",
       "0    02a-01  Illinois  Illinois Geospatial Data Clearinghouse   \n",
       "1    02a-03  Illinois           State of Illinois Data Portal   \n",
       "2    01d-02   Indiana             Indiana Spatial Data Portal   \n",
       "3    09a-01   Indiana                              IndianaMAP   \n",
       "4    03a-02      Iowa                       IowaDOT Open Data   \n",
       "\n",
       "                                      sourceURL                      btaaURL  \n",
       "0  https://clearinghouse.isgs.illinois.edu/data  https://geo.btaa.org/?q=02a  \n",
       "1                     https://data.illinois.gov  https://geo.btaa.org/?q=02a  \n",
       "2                             http://gis.iu.edu  https://geo.btaa.org/?q=01d  \n",
       "3     http://maps.indiana.edu/layerGallery.html  https://geo.btaa.org/?q=09a  \n",
       "4    https://public-iowadot.opendata.arcgis.com  https://geo.btaa.org/?q=03a  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_csv = pd.read_csv(stategeoportals)\n",
    "df_csv['btaaURL'] = df_csv['btaaURL'].apply(lambda x: x.split('-')[0])\n",
    "df_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etract total records number from BTAA Geoportal search page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def totalRecords(df):\n",
    "    totalrecords = []\n",
    "    for _, row in df.iterrows():\n",
    "        url = row['btaaURL']        \n",
    "        ## Start session and get the search page\n",
    "        session = requests.Session()\n",
    "        response = session.get(url)\n",
    "        ## Parse only part of the page (<meta> tag) for better performance using SoupStrainer and lxml\n",
    "        strainer = SoupStrainer('meta', attrs={'name': 'totalResults'})\n",
    "        soup = BeautifulSoup(response.content, 'lxml', parse_only=strainer)\n",
    "        ## The find() method looks through <meta> tagâ€™s descendants and retrieves one result with attribute 'name'\n",
    "        meta_tag = soup.find('meta', attrs={'name': 'totalResults'})\n",
    "        ## Grab the content inside the <meta> tag that matches the filter\n",
    "        totalrecord = meta_tag.get('content')\n",
    "        totalrecords.append(totalrecord)\n",
    "    return totalrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='geo.btaa.org', port=443): Max retries exceeded with url: /?q=02a (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x000001E25275B788>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    156\u001b[0m             conn = connection.create_connection(\n\u001b[1;32m--> 157\u001b[1;33m                 \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[0maf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    751\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 752\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    753\u001b[0m         \u001b[0maf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    671\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 672\u001b[1;33m                 \u001b[0mchunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    673\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    375\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m    993\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sock\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 994\u001b[1;33m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    995\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    299\u001b[0m         \u001b[1;31m# Add certificate verification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m         \u001b[0mconn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    168\u001b[0m             raise NewConnectionError(\n\u001b[1;32m--> 169\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Failed to establish a new connection: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m             )\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.VerifiedHTTPSConnection object at 0x000001E25275B788>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m                 )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    719\u001b[0m             retries = retries.increment(\n\u001b[1;32m--> 720\u001b[1;33m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    721\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py\u001b[0m in \u001b[0;36mincrement\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    435\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 436\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='geo.btaa.org', port=443): Max retries exceeded with url: /?q=02a (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x000001E25275B788>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-9b71701719ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_csv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'totalRecords'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotalRecords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_csv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-267a28a3fc3e>\u001b[0m in \u001b[0;36mtotalRecords\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;31m## Start session and get the search page\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[1;31m## Parse only part of the page (<meta> tag) for better performance using SoupStrainer and lxml\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mstrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSoupStrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'meta'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'totalResults'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 546\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GET'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    547\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    531\u001b[0m         }\n\u001b[0;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 646\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    514\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='geo.btaa.org', port=443): Max retries exceeded with url: /?q=02a (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x000001E25275B788>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))"
     ]
    }
   ],
   "source": [
    "df_csv['totalRecords'] = totalRecords(df_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the numinum number of total records\n",
    "If it equals to 0, meaning the landing page is 404 Not Found. Go back to check if the identifier is still active. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_totalRecords(df):\n",
    "    df['totalRecords'] = df['totalRecords'].astype(int)\n",
    "    if df['totalRecords'].min() == 0:\n",
    "        return df[df['totalRecords']==0]\n",
    "    else:\n",
    "        print('> State Geoportal Codes all valid!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_totalRecords(df_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group dataframe rows into list by geoportal sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_to_array(data):\n",
    "    groupItems = ['stateCode', 'Title', 'sourceURL']\n",
    "    for i in range(len(groupItems)):\n",
    "        data[groupItems[i]] = np.tile([data[groupItems[i]].values], (data.shape[0], 1)).tolist()\n",
    "    return data\n",
    "\n",
    "## Group by ['State']\n",
    "df_group = df_csv.groupby(['State']).apply(aggregate_to_array).drop_duplicates(subset=['State'])\n",
    "df_group.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge state GeoJSON and geoportal GeoJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load statejson featuer properties\n",
    "state_geojson = gpd.read_file(statejson)\n",
    "state_json = json.loads(state_geojson.to_json())\n",
    "df_allState = pd.json_normalize(state_json['features'])\n",
    "\n",
    "## Change column names for further operation\n",
    "df_allState = df_allState[['properties.State', 'geometry.coordinates']].rename(\n",
    "    columns={'properties.State':'State', 'geometry.coordinates':'boundingBox'})\n",
    "\n",
    "df_allState.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Join on column 'State' from left dataframe df_group\n",
    "df_merge = pd.merge(df_group, df_allState, on = 'State', how = 'left')\n",
    "df_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create state GeoJSON features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_geojson_features(df):\n",
    "    print('> Creating state GeoJSON features...')\n",
    "    features = []\n",
    "    geometry_type = ''\n",
    "    geojson = {\n",
    "        'type': 'FeatureCollection',\n",
    "        'features': features\n",
    "    }\n",
    "        \n",
    "    for _, row in df.iterrows():\n",
    "        if type(row['boundingBox'][0][0][0]) is float:\n",
    "            geometry_type = 'Polygon'\n",
    "        else:\n",
    "            geometry_type = 'MultiPolygon'\n",
    "            \n",
    "        feature = {\n",
    "            'type': 'Feature',\n",
    "            'geometry': {\n",
    "                'type': geometry_type, \n",
    "                'coordinates': row['boundingBox']\n",
    "            },\n",
    "            'properties': {\n",
    "                'State': row['State'],\n",
    "                'Title': '|'.join([str(elem) for elem in row['Title']]),\n",
    "                'sourceURL': '|'.join([str(elem) for elem in row['sourceURL']]), \n",
    "                'btaaURL': row['btaaURL'],\n",
    "                'totalRecords': row['totalRecords']\n",
    "            }\n",
    "           }\n",
    "\n",
    "        features.append(feature)\n",
    "    return geojson\n",
    "\n",
    "data_geojson = create_geojson_features(df_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to state TopoJSON file `activeStates.topo.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_geojson = gpd.GeoDataFrame.from_features(data_geojson[\"features\"])\n",
    "# TopoJSON is an extension of GeoJSON to compress geometry information\n",
    "topo = tp.Topology(state_geojson)\n",
    "topo.to_json(activestates)\n",
    "print('> Creating state TopoJSON file...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect state TopoJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topo.to_alt().properties(title='State Topology')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Part 2: County Geoportals TopoJSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format county name in county geoportals spreadsheet `allCounties.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = pd.read_csv(countygeoportals)\n",
    "\n",
    "## Replace 'Saint' and 'St' with 'St.'\n",
    "df_csv['County'] = df_csv['County'].apply(lambda x: re.sub(r'(Saint\\s|^St\\s|^St\\.\\s)', 'St. ', x))\n",
    "\n",
    "df_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etract total records number from BTAA Geoportal search page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv['totalRecords'] = totalRecords(df_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the numinum number of total records\n",
    "If it equals to 0, meaning the landing page is 404 Not Found. Go back to check if the identifier is still active. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_totalRecords(df_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group dataframe rows into list by geoportal sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_to_array(data):\n",
    "    groupItems = ['Title', 'sourceURL', 'totalRecords']\n",
    "    for i in range(len(groupItems)):\n",
    "        data[groupItems[i]] = np.tile([data[groupItems[i]].values], (data.shape[0], 1)).tolist()\n",
    "    return data\n",
    "\n",
    "## Group by ['County', 'State']\n",
    "df_group = df_csv.groupby(['County', 'State']).apply(aggregate_to_array).drop_duplicates(subset=['County', 'State'])\n",
    "## Sum up the total records if there're multiple geoportals in one county\n",
    "df_group['totalRecords'] = df_group['totalRecords'].apply(lambda x: sum(int(item)for item in x))\n",
    "df_group.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify the geoportal by total number\n",
    "You may want to adjust the classification method **`Quantiles`** and class number **`k`**. \n",
    "Reference the <a href=\"https://pypi.org/project/mapclassify/\">mapclassify</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n5 = mapclassify.Quantiles(df_group.totalRecords, k=5)\n",
    "n5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countyInterval = list(n5.bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign different color to each geoportal based on total records class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select the gradient color palette\n",
    "palette = ['#b3cde0','#6497b1','#005b96','#03396c ','#011f4b']\n",
    "colorScale = np.array(['#b3cde0','#6497b1','#005b96','#03396c','#011f4b'])\n",
    "sns.palplot(sns.color_palette(colorScale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def totalRecords_color(row):\n",
    "    if row['totalRecords'] <= countyInterval[0]:\n",
    "        return palette[0]\n",
    "    elif row['totalRecords'] > countyInterval[0] and row['totalRecords'] <= countyInterval[1]:\n",
    "        return palette[1]\n",
    "    elif row['totalRecords'] > countyInterval[1] and row['totalRecords'] <= countyInterval[2]:\n",
    "        return palette[2]\n",
    "    elif row['totalRecords'] > countyInterval[2] and row['totalRecords'] <= countyInterval[3]:\n",
    "        return palette[3]\n",
    "    else:\n",
    "        return palette[4]\n",
    "\n",
    "## Append a new column with color generated above\n",
    "df_group['Color'] = df_group.apply(totalRecords_color, axis=1)\n",
    "df_group.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge county GeoJSON and geoportal GeoJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load countyjson featuer properties\n",
    "county_geojson = gpd.read_file(countyjson)\n",
    "county_json = json.loads(county_geojson.to_json())\n",
    "df_allCounty = pd.json_normalize(county_json['features'])\n",
    "\n",
    "## Change column names for further operation\n",
    "df_allCounty = df_allCounty[['properties.County', 'properties.State', 'geometry.coordinates']].rename(\n",
    "    columns={'properties.County':'County', 'properties.State':'State', 'geometry.coordinates':'boundingBox'})\n",
    "\n",
    "df_allCounty.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Join on column 'County' and 'State' from left dataframe df_group\n",
    "df_merge = pd.merge(df_group, df_allCounty, on = ['County','State'], how = 'left')\n",
    "df_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return rows with Nan value\n",
    "Check if there exists any records doesn't include any coordinates information in the boundingBox column. <br>\n",
    "If so, go back to `allCounties.csv` and manually change the **county** name to the one in `county.json`, then go to `Kernel` > `Restart & Run All`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nanrows(df):\n",
    "    if df.isnull().values.any():\n",
    "        return df[df['boundingBox'].isnull()]\n",
    "    else:\n",
    "        print('> No NULL rows')        \n",
    "check_nanrows(df_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create county GeoJSON features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_geojson_features(df):\n",
    "    print('> Creating county GeoJSON features...')\n",
    "    features = []\n",
    "    geometry_type = ''\n",
    "    geojson = {\n",
    "        'type': 'FeatureCollection',\n",
    "        'features': features\n",
    "    }\n",
    "        \n",
    "    for _, row in df.iterrows():\n",
    "        if type(row['boundingBox'][0][0][0]) is float:\n",
    "            geometry_type = 'Polygon'\n",
    "        else:\n",
    "            geometry_type = 'MultiPolygon'\n",
    "            \n",
    "        feature = {\n",
    "            'type': 'Feature',\n",
    "            'geometry': {\n",
    "                'type': geometry_type, \n",
    "                'coordinates': row['boundingBox']\n",
    "            },\n",
    "            'properties': {\n",
    "                'County': row['County'],\n",
    "                'State': row['State'],\n",
    "                'countyCode': row['countyCode'],\n",
    "                'Title': '|'.join([str(elem) for elem in row['Title']]),\n",
    "                'sourceURL': '|'.join([str(elem) for elem in row['sourceURL']]), \n",
    "                'btaaURL': row['btaaURL'],\n",
    "                'totalRecords': row['totalRecords'],\n",
    "                'Color' : row['Color']\n",
    "            }\n",
    "           }\n",
    "\n",
    "        features.append(feature)\n",
    "    return geojson\n",
    "\n",
    "data_geojson = create_geojson_features(df_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to county TopoJSON file `activeCounties.topo.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "county_geojson = gpd.GeoDataFrame.from_features(data_geojson[\"features\"])\n",
    "# TopoJSON is an extension of GeoJSON to compress geometry information\n",
    "topo = tp.Topology(county_geojson)\n",
    "topo.to_json(activecounties)\n",
    "print('> Creating county TopoJSON file...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect county TopoJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topo.to_alt().properties(title='County Topology')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Part 3: City Geoportals GeoJSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format city name in city geoportals spreadsheet `allCities.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(citygeoportals)\n",
    "## Calculate city coordinates and round to 2 decimal places \n",
    "df = pd.concat([df, df['Bounding Box'].str.split(',', expand=True).astype(float)], axis=1).rename(\n",
    "    columns={0:'minX', 1:'minY', 2:'maxX', 3:'maxY'})\n",
    "df['centerX'] = round((df['minX'] + df['maxX']) / 2, 2)\n",
    "df['centerY'] = round((df['minY'] + df['maxY']) / 2, 2)\n",
    "df_clean = df.drop(columns =['minX', 'minY', 'maxX', 'maxY', 'Bounding Box'])\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etract total records number from BTAA Geoportal search page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['totalRecords'] = totalRecords(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the numinum number of total records\n",
    "If it equals to 0, meaning the landing page is 404 Not Found. Go back to check if the identifier is still active. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_totalRecords(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group dataframe rows into list by geoportal sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_to_array(data):\n",
    "    groupItems = ['Title', 'sourceURL', 'totalRecords']\n",
    "    for i in range(len(groupItems)):\n",
    "        data[groupItems[i]] = np.tile([data[groupItems[i]].values], (data.shape[0], 1)).tolist()\n",
    "    return data\n",
    "\n",
    "## Group by ['City', 'State']\n",
    "df_group = df_clean.groupby(['centerX']).apply(aggregate_to_array).drop_duplicates(subset=['City', 'State'])\n",
    "# sum up the total records if there're multiple geoportals in one city\n",
    "df_group['totalRecords'] = df_group['totalRecords'].apply(lambda x: sum(int(item)for item in x))\n",
    "df_group.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify the geoportal by total number\n",
    "You may want to adjust the classification method **`Quantiles`** and class number **`k`**. \n",
    "Reference the <a href=\"https://pypi.org/project/mapclassify/\">mapclassify</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n3 = mapclassify.Quantiles(df_group.totalRecords, k=3)\n",
    "n3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cityInterval = list(n3.bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign different circle radius to each geoportal based on total records class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Size of symbols on map in meters \n",
    "size = [12000, 16000, 22000]\n",
    "## Size of symbols inside legend in pixels\n",
    "legendSize = [12, 18, 28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def totalRecords_size(row):\n",
    "    if row['totalRecords'] <= cityInterval[0]:\n",
    "        return size[0]\n",
    "    elif row['totalRecords'] > cityInterval[0] and row['totalRecords'] <= cityInterval[1]:\n",
    "        return size[1]\n",
    "    else:\n",
    "        return size[2]\n",
    "\n",
    "## Append a new column with color generated above\n",
    "df_group['Size'] = df_group.apply(totalRecords_size, axis=1)\n",
    "df_group.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create city GeoJSON features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_geojson_features(df):\n",
    "    print('> Creating city GeoJSON features...')\n",
    "    features = []\n",
    "    geojson = {\n",
    "        'type': 'FeatureCollection',\n",
    "        'features': features\n",
    "    }\n",
    "    for _, row in df.iterrows():\n",
    "        feature = {\n",
    "            'type': 'Feature',\n",
    "            'geometry': {\n",
    "                'type':'Point', \n",
    "                'coordinates':[row['centerX'], row['centerY']]\n",
    "            },\n",
    "            'properties': {\n",
    "                'City': row['City'],\n",
    "                'State': row['State'],\n",
    "                'Title': '|'.join([str(elem) for elem in row['Title']]),\n",
    "                'sourceURL': '|'.join([str(elem) for elem in row['sourceURL']]), \n",
    "                'btaaURL': row['btaaURL'],\n",
    "                'totalRecords': row['totalRecords'],\n",
    "                'Size' : row['Size']\n",
    "            }\n",
    "           }\n",
    "\n",
    "        features.append(feature)\n",
    "    return geojson\n",
    "\n",
    "data_geojson = create_geojson_features(df_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to city GeoJSON file `activecities.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(activecities, 'w') as txtfile:\n",
    "    json.dump(data_geojson, txtfile)\n",
    "print('> Creating city GeoJSON file...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect city GeoJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('> Making map...')\n",
    "m = folium.Map(location = [44, -90], control_scale = True, zoom_start = 6)\n",
    "\n",
    "lyrCity = folium.GeoJson(open(activecities, 'r').read(),\n",
    "               tooltip = folium.GeoJsonTooltip(fields=('City', 'sourceURL'),\n",
    "                                               aliases=('city','websiteUrl')),\n",
    "               show = True).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Part 4: Legend JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create legend JSON features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_legend_json(countyinterval, palette, cityinterval, size):\n",
    "    print('> Creating legend JSON featuers...')\n",
    "    countystyle = dict(zip(countyinterval, palette))\n",
    "    citystyle = dict(zip(cityinterval, size))\n",
    "    dic = {\n",
    "            'county':countystyle,\n",
    "              'city':citystyle\n",
    "          }\n",
    "    return dic\n",
    "    \n",
    "data_json = create_legend_json(countyInterval, palette, cityInterval, legendSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to legend JSON file `legend.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(legendjson, 'w') as txtfile:\n",
    "    json.dump(data_json, txtfile)\n",
    "print('> Creating legend JSON file...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
